{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ac39b2",
   "metadata": {},
   "source": [
    "#### Converting Zip files (okutama-action/TrainSetFrames.zip) and (okutama-action/TestSetFrames.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbde0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41ab07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# # Paths (using raw strings to avoid escape issues)\n",
    "# zip_path = r\"okutama-action/TrainSetFrames.zip\"\n",
    "# extract_dir = r\"okutama-action/TrainSetFrames\"\n",
    "\n",
    "# # Create directory if it doesn't exist\n",
    "# os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# # Extract ZIP\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_dir)\n",
    "\n",
    "# # Count number of files extracted\n",
    "# num_files = sum(len(files) for _, _, files in os.walk(extract_dir))\n",
    "# print(f\"Frames extracted to: {extract_dir}\")\n",
    "# print(f\"Total number of frames extracted: {num_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "664f9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# # Paths (using raw strings to avoid escape issues)\n",
    "# zip_path = r\"okutama-action/TestSetFrames.zip\"\n",
    "# extract_dir = r\"okutama-action/TestSetFrames\"\n",
    "\n",
    "# # Create directory if it doesn't exist\n",
    "# os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# # Extract ZIP\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_dir)\n",
    "\n",
    "# # Count number of files extracted\n",
    "# num_files = sum(len(files) for _, _, files in os.walk(extract_dir))\n",
    "# print(f\"Frames extracted to: {extract_dir}\")\n",
    "# print(f\"Total number of frames extracted: {num_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bc3f2",
   "metadata": {},
   "source": [
    "#### Visualizing a converted zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f052e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e27cc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Paths\n",
    "# frames_root = r\"okutama-action/TrainSetFrames\"\n",
    "# labels_dir = r\"okutama-action/TrainSetFrames/Labels/SingleActionLabels/3840x2160\"\n",
    "\n",
    "# # Original label resolution (always 4K)\n",
    "# ORIG_W, ORIG_H = 3840, 2160\n",
    "\n",
    "# # Number of sample frames to display\n",
    "# max_samples = 1\n",
    "# sample_count = 0\n",
    "\n",
    "# # Define colors for actions\n",
    "# ACTION_COLORS = {\n",
    "#     \"Handshaking\": (255, 0, 0),       # Red\n",
    "#     \"Hugging\": (0, 0, 255),           # Blue\n",
    "#     \"Reading\": (255, 255, 0),         # Cyan\n",
    "#     \"Drinking\": (0, 255, 255),        # Yellow\n",
    "#     \"Pushing/Pulling\": (255, 0, 255), # Magenta\n",
    "#     \"Carrying\": (128, 0, 128),        # Purple\n",
    "#     \"Calling\": (0, 128, 255),         # Orange\n",
    "#     \"Running\": (0, 255, 0),           # Green\n",
    "#     \"Walking\": (128, 128, 0),         # Olive\n",
    "#     \"Lying\": (128, 0, 0),             # Maroon\n",
    "#     \"Sitting\": (0, 128, 128),         # Teal\n",
    "#     \"Standing\": (192, 192, 192),      # Gray\n",
    "#     \"O\": (255, 192, 203),             # Pink\n",
    "#     \"Okutama-Action\": (0, 0, 0),      # Black\n",
    "#     \"None-Interaction\": (128, 128, 128) # Dark Gray\n",
    "# }\n",
    "\n",
    "# DEFAULT_COLOR = (0, 255, 0)  # Bright green if action not in map\n",
    "\n",
    "# def draw_bboxes(img, label_lines, frame_number, max_boxes_per_frame=10):\n",
    "#     h, w, _ = img.shape\n",
    "#     boxes_drawn = 0\n",
    "#     seen_actions = set()  # keep one box per action type\n",
    "\n",
    "#     for line in label_lines:\n",
    "#         if boxes_drawn >= max_boxes_per_frame:\n",
    "#             break\n",
    "\n",
    "#         parts = line.strip().split()\n",
    "#         if len(parts) < 11:\n",
    "#             continue\n",
    "\n",
    "#         # Parse values\n",
    "#         xmin, ymin, xmax, ymax = map(float, parts[1:5])\n",
    "#         line_frame = int(parts[5])\n",
    "#         lost = int(parts[6])\n",
    "#         occluded = int(parts[7])\n",
    "#         label_class = parts[9].strip('\"')\n",
    "#         action = parts[10].strip('\"') if len(parts) > 10 else \"\"\n",
    "\n",
    "#         if line_frame != frame_number:\n",
    "#             continue\n",
    "#         if lost == 1:\n",
    "#             continue\n",
    "\n",
    "#         # one box per action per frame\n",
    "#         # if action in seen_actions:\n",
    "#         #     continue\n",
    "#         # seen_actions.add(action)\n",
    "\n",
    "#         # Rescale coordinates to frame\n",
    "#         x1 = int(round(xmin * (w / ORIG_W)))\n",
    "#         y1 = int(round(ymin * (h / ORIG_H)))\n",
    "#         x2 = int(round(xmax * (w / ORIG_W)))\n",
    "#         y2 = int(round(ymax * (h / ORIG_H)))\n",
    "\n",
    "#         # Clip\n",
    "#         x1, y1 = max(0, x1), max(0, y1)\n",
    "#         x2, y2 = min(w - 1, x2), min(h - 1, y2)\n",
    "#         if x2 <= x1 or y2 <= y1:\n",
    "#             continue\n",
    "\n",
    "#         # Pick color for action\n",
    "#         color = ACTION_COLORS.get(action, DEFAULT_COLOR)\n",
    "\n",
    "#         # Draw bbox + label\n",
    "#         cv2.rectangle(img, (x1, y1), (x2, y2), color, 1)\n",
    "#         cv2.putText(img, f\"{label_class}-{action}\", (x1, max(y1 - 10, 0)),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "#         boxes_drawn += 1\n",
    "\n",
    "#     return img\n",
    "\n",
    "\n",
    "# # Walk dataset\n",
    "# for drone in sorted(os.listdir(frames_root)):\n",
    "#     drone_path = os.path.join(frames_root, drone)\n",
    "#     if not os.path.isdir(drone_path):\n",
    "#         continue\n",
    "\n",
    "#     for time_dir in sorted(os.listdir(drone_path)):\n",
    "#         time_path = os.path.join(drone_path, time_dir)\n",
    "#         if not os.path.isdir(time_path):\n",
    "#             continue\n",
    "\n",
    "#         extracted_path = os.path.join(time_path, \"Extracted-Frames-1280x720\")\n",
    "#         if not os.path.isdir(extracted_path):\n",
    "#             continue\n",
    "\n",
    "#         for video_folder in sorted(os.listdir(extracted_path)):\n",
    "#             video_path = os.path.join(extracted_path, video_folder)\n",
    "#             if not os.path.isdir(video_path):\n",
    "#                 continue\n",
    "\n",
    "#             # Label file\n",
    "#             label_file = os.path.join(labels_dir, f\"{video_folder}.txt\")\n",
    "#             if not os.path.exists(label_file):\n",
    "#                 print(f\"No label file for {video_folder}\")\n",
    "#                 continue\n",
    "#             with open(label_file, 'r') as lf:\n",
    "#                 label_lines = lf.readlines()\n",
    "\n",
    "#             # Sort frames\n",
    "#             frame_files = sorted(\n",
    "#                 [f for f in os.listdir(video_path) if f.lower().endswith(('.jpg', '.png'))],\n",
    "#                 key=lambda x: int(os.path.splitext(x)[0])\n",
    "#             )\n",
    "\n",
    "#             for f in frame_files:\n",
    "#                 frame_number = int(os.path.splitext(f)[0])\n",
    "#                 img_path = os.path.join(video_path, f)\n",
    "#                 img = cv2.imread(img_path)\n",
    "#                 if img is None:\n",
    "#                     continue\n",
    "#                 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#                 # Draw bboxes\n",
    "#                 img = draw_bboxes(img, label_lines, frame_number)\n",
    "\n",
    "#                 plt.figure(figsize=(16, 9))\n",
    "#                 plt.imshow(img)\n",
    "#                 plt.title(f\"{drone}/{time_dir}/{video_folder} Frame {frame_number}\")\n",
    "#                 plt.axis('off')\n",
    "#                 plt.show()\n",
    "\n",
    "#                 sample_count += 1\n",
    "#                 if sample_count >= max_samples:\n",
    "#                     break\n",
    "#             if sample_count >= max_samples:\n",
    "#                 break\n",
    "#         if sample_count >= max_samples:\n",
    "#             break\n",
    "#     if sample_count >= max_samples:\n",
    "#         break\n",
    "\n",
    "# print(\"Finished displaying frames with colored bboxes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9766a9d",
   "metadata": {},
   "source": [
    "#### Yolo-format conversion train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce3920a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "\n",
    "# # Paths\n",
    "# frames_root = r\"okutama-action/TestSetFrames\"  # Test set frames\n",
    "# labels_dir = r\"okutama-action/TestSetFrames/Labels/SingleActionLabels/3840x2160\"\n",
    "# output_root = r\"dataset-yolo\"\n",
    "\n",
    "# # Original label resolution\n",
    "# ORIG_W, ORIG_H = 3840, 2160\n",
    "\n",
    "# # Output structure\n",
    "# os.makedirs(os.path.join(output_root, \"images\", \"test\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(output_root, \"labels\", \"test\"), exist_ok=True)\n",
    "\n",
    "# # Class mapping\n",
    "# CLASS_NAMES = [\n",
    "#     \"Handshaking\", \"Hugging\", \"Reading\", \"Drinking\", \"PushingPulling\",\n",
    "#     \"Carrying\", \"Calling\", \"Running\", \"Walking\", \"Lying\",\n",
    "#     \"Sitting\", \"Standing\", \"None\"\n",
    "# ]\n",
    "# CLASS2ID = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# def convert_to_yolo(x1, y1, x2, y2, img_w, img_h):\n",
    "#     bw = x2 - x1\n",
    "#     bh = y2 - y1\n",
    "#     cx = x1 + bw / 2\n",
    "#     cy = y1 + bh / 2\n",
    "#     return cx / img_w, cy / img_h, bw / img_w, bh / img_h\n",
    "\n",
    "# def process_label_line(parts, frame_number, img_w, img_h):\n",
    "#     if len(parts) < 11:\n",
    "#         return None\n",
    "#     line_frame = int(parts[5])\n",
    "#     if line_frame != frame_number:\n",
    "#         return None\n",
    "#     x1 = int(round(float(parts[1]) * (img_w / ORIG_W)))\n",
    "#     y1 = int(round(float(parts[2]) * (img_h / ORIG_H)))\n",
    "#     x2 = int(round(float(parts[3]) * (img_w / ORIG_W)))\n",
    "#     y2 = int(round(float(parts[4]) * (img_h / ORIG_H)))\n",
    "#     x1, y1 = max(0, x1), max(0, y1)\n",
    "#     x2, y2 = min(img_w - 1, x2), min(img_h - 1, y2)\n",
    "#     if x2 <= x1 or y2 <= y1:\n",
    "#         return None\n",
    "#     label_class = parts[-2].strip('\"')\n",
    "#     action = parts[-1].strip('\"')\n",
    "#     if action not in CLASS2ID:\n",
    "#         return None\n",
    "#     cx, cy, bw, bh = convert_to_yolo(x1, y1, x2, y2, img_w, img_h)\n",
    "#     return f\"{CLASS2ID[action]} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\"\n",
    "\n",
    "# # ---- Main loop ----\n",
    "# all_samples = []\n",
    "\n",
    "# for drone in sorted(os.listdir(frames_root)):\n",
    "#     drone_path = os.path.join(frames_root, drone)\n",
    "#     if not os.path.isdir(drone_path):\n",
    "#         continue\n",
    "\n",
    "#     for time_dir in sorted(os.listdir(drone_path)):\n",
    "#         time_path = os.path.join(drone_path, time_dir)\n",
    "#         extracted_path = os.path.join(time_path, \"Extracted-Frames-1280x720\")\n",
    "#         if not os.path.isdir(extracted_path):\n",
    "#             continue\n",
    "\n",
    "#         for video_folder in sorted(os.listdir(extracted_path)):\n",
    "#             video_path = os.path.join(extracted_path, video_folder)\n",
    "#             if not os.path.isdir(video_path):\n",
    "#                 continue\n",
    "\n",
    "#             label_file = os.path.join(labels_dir, f\"{video_folder}.txt\")\n",
    "#             if not os.path.exists(label_file):\n",
    "#                 print(f\"No label file for {video_folder}\")\n",
    "#                 continue\n",
    "#             with open(label_file, \"r\") as lf:\n",
    "#                 label_lines = lf.readlines()\n",
    "\n",
    "#             frame_files = sorted(\n",
    "#                 [f for f in os.listdir(video_path) if f.lower().endswith((\".jpg\", \".png\"))],\n",
    "#                 key=lambda x: int(os.path.splitext(x)[0])\n",
    "#             )\n",
    "\n",
    "#             for f in frame_files:\n",
    "#                 frame_number = int(os.path.splitext(f)[0])\n",
    "#                 img_path = os.path.join(video_path, f)\n",
    "#                 img = cv2.imread(img_path)\n",
    "#                 if img is None:\n",
    "#                     continue\n",
    "#                 img_h, img_w = img.shape[:2]\n",
    "\n",
    "#                 yolo_lines = []\n",
    "#                 for line in label_lines:\n",
    "#                     parts = line.strip().split()\n",
    "#                     ann = process_label_line(parts, frame_number, img_w, img_h)\n",
    "#                     if ann:\n",
    "#                         yolo_lines.append(ann)\n",
    "\n",
    "#                 if not yolo_lines:\n",
    "#                     continue\n",
    "\n",
    "#                 all_samples.append((img_path, yolo_lines))\n",
    "\n",
    "# # ---- Write test output ----\n",
    "# for i, (img_path, yolo_lines) in enumerate(all_samples):\n",
    "#     fname = f\"test_{i:06d}.jpg\"\n",
    "#     out_img_path = os.path.join(output_root, \"images\", \"test\", fname)\n",
    "#     out_lbl_path = os.path.join(output_root, \"labels\", \"test\", fname.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "#     img = cv2.imread(img_path)\n",
    "#     cv2.imwrite(out_img_path, img)\n",
    "\n",
    "#     with open(out_lbl_path, \"w\") as f:\n",
    "#         f.write(\"\\n\".join(yolo_lines))\n",
    "\n",
    "# print(\"✅ Test set conversion to YOLO format finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8b9790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import random\n",
    "\n",
    "# # ------------------- Paths -------------------\n",
    "# frames_root = r\"okutama-action/TrainSetFrames\"\n",
    "# labels_dir = r\"okutama-action/TrainSetFrames/Labels/SingleActionLabels/3840x2160\"\n",
    "# output_root = r\"dataset-yolo\"\n",
    "\n",
    "# # Original label resolution\n",
    "# ORIG_W, ORIG_H = 3840, 2160\n",
    "\n",
    "# # Output directories\n",
    "# splits = [\"train\", \"val\"]\n",
    "# for s in splits:\n",
    "#     os.makedirs(os.path.join(output_root, \"images\", s), exist_ok=True)\n",
    "#     os.makedirs(os.path.join(output_root, \"labels\", s), exist_ok=True)\n",
    "\n",
    "# # ------------------- Classes -------------------\n",
    "# CLASS_NAMES = [\n",
    "#     \"Handshaking\", \"Hugging\", \"Reading\", \"Drinking\", \"PushingPulling\",\n",
    "#     \"Carrying\", \"Calling\", \"Running\", \"Walking\", \"Lying\",\n",
    "#     \"Sitting\", \"Standing\", \"None\"\n",
    "# ]\n",
    "# CLASS2ID = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# # ------------------- Functions -------------------\n",
    "# def convert_to_yolo(x1, y1, x2, y2, img_w, img_h):\n",
    "#     \"\"\"Convert bbox to YOLO format (normalized cx, cy, w, h).\"\"\"\n",
    "#     bw = x2 - x1\n",
    "#     bh = y2 - y1\n",
    "#     cx = x1 + bw / 2\n",
    "#     cy = y1 + bh / 2\n",
    "#     return cx / img_w, cy / img_h, bw / img_w, bh / img_h\n",
    "\n",
    "# def process_label_line(parts, frame_number, img_w, img_h):\n",
    "#     \"\"\"Parse one annotation line and return YOLO string if valid.\"\"\"\n",
    "#     if len(parts) < 11:\n",
    "#         return None\n",
    "\n",
    "#     line_frame = int(parts[5])  # frame column in SingleActionLabels\n",
    "#     if line_frame != frame_number:\n",
    "#         return None\n",
    "\n",
    "#     x1 = int(round(float(parts[1]) * (img_w / ORIG_W)))\n",
    "#     y1 = int(round(float(parts[2]) * (img_h / ORIG_H)))\n",
    "#     x2 = int(round(float(parts[3]) * (img_w / ORIG_W)))\n",
    "#     y2 = int(round(float(parts[4]) * (img_h / ORIG_H)))\n",
    "\n",
    "#     # Clip bounding box to image\n",
    "#     x1, y1 = max(0, x1), max(0, y1)\n",
    "#     x2, y2 = min(img_w - 1, x2), min(img_h - 1, y2)\n",
    "#     if x2 <= x1 or y2 <= y1:\n",
    "#         return None\n",
    "\n",
    "#     label_class = parts[-2].strip('\"')\n",
    "#     action = parts[-1].strip('\"')\n",
    "#     if action not in CLASS2ID:\n",
    "#         return None\n",
    "\n",
    "#     # Convert to YOLO format\n",
    "#     cx, cy, bw, bh = convert_to_yolo(x1, y1, x2, y2, img_w, img_h)\n",
    "#     return f\"{CLASS2ID[action]} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\"\n",
    "\n",
    "# # ------------------- Main Loop -------------------\n",
    "# all_samples = []\n",
    "\n",
    "# for drone in sorted(os.listdir(frames_root)):\n",
    "#     drone_path = os.path.join(frames_root, drone)\n",
    "#     if not os.path.isdir(drone_path):\n",
    "#         continue\n",
    "\n",
    "#     for time_dir in sorted(os.listdir(drone_path)):\n",
    "#         time_path = os.path.join(drone_path, time_dir)\n",
    "#         extracted_path = os.path.join(time_path, \"Extracted-Frames-1280x720\")\n",
    "#         if not os.path.isdir(extracted_path):\n",
    "#             continue\n",
    "\n",
    "#         for video_folder in sorted(os.listdir(extracted_path)):\n",
    "#             video_path = os.path.join(extracted_path, video_folder)\n",
    "#             if not os.path.isdir(video_path):\n",
    "#                 continue\n",
    "\n",
    "#             label_file = os.path.join(labels_dir, f\"{video_folder}.txt\")\n",
    "#             if not os.path.exists(label_file):\n",
    "#                 print(f\"No label file for {video_folder}\")\n",
    "#                 continue\n",
    "#             with open(label_file, \"r\") as lf:\n",
    "#                 label_lines = lf.readlines()\n",
    "\n",
    "#             frame_files = sorted(\n",
    "#                 [f for f in os.listdir(video_path) if f.lower().endswith((\".jpg\", \".png\"))],\n",
    "#                 key=lambda x: int(os.path.splitext(x)[0])\n",
    "#             )\n",
    "\n",
    "#             for f in frame_files:\n",
    "#                 frame_number = int(os.path.splitext(f)[0])\n",
    "#                 img_path = os.path.join(video_path, f)\n",
    "#                 img = cv2.imread(img_path)\n",
    "#                 if img is None:\n",
    "#                     continue\n",
    "#                 img_h, img_w = img.shape[:2]\n",
    "\n",
    "#                 yolo_lines = []\n",
    "#                 for line in label_lines:\n",
    "#                     parts = line.strip().split()\n",
    "#                     ann = process_label_line(parts, frame_number, img_w, img_h)\n",
    "#                     if ann:\n",
    "#                         yolo_lines.append(ann)\n",
    "\n",
    "#                 if not yolo_lines:\n",
    "#                     continue\n",
    "\n",
    "#                 all_samples.append((img_path, yolo_lines))\n",
    "\n",
    "# # ------------------- Split Train / Val -------------------\n",
    "# random.shuffle(all_samples)\n",
    "# n = len(all_samples)\n",
    "# train_split = int(0.7 * n)\n",
    "\n",
    "# splits_idx = {\n",
    "#     \"train\": all_samples[:train_split],\n",
    "#     \"val\": all_samples[train_split:]\n",
    "# }\n",
    "\n",
    "# # ------------------- Write Output -------------------\n",
    "# for split, samples in splits_idx.items():\n",
    "#     for i, (img_path, yolo_lines) in enumerate(samples):\n",
    "#         fname = f\"{split}_{i:06d}.jpg\"\n",
    "#         out_img_path = os.path.join(output_root, \"images\", split, fname)\n",
    "#         out_lbl_path = os.path.join(output_root, \"labels\", split, fname.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "#         # Copy image\n",
    "#         img = cv2.imread(img_path)\n",
    "#         cv2.imwrite(out_img_path, img)\n",
    "\n",
    "#         # Write YOLO labels\n",
    "#         with open(out_lbl_path, \"w\") as f:\n",
    "#             f.write(\"\\n\".join(yolo_lines))\n",
    "\n",
    "# print(\"✅ YOLO dataset for train & val created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c242b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Images = 39835, Labels = 39835\n",
      "VAL: Images = 17073, Labels = 17073\n",
      "TEST: Images = 15698, Labels = 15698\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_root = \"dataset-yolo\"\n",
    "splits = [\"train\", \"val\", \"test\"]  # include test if you have it\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = os.path.join(dataset_root, \"images\", split)\n",
    "    lbl_dir = os.path.join(dataset_root, \"labels\", split)\n",
    "\n",
    "    if not os.path.exists(img_dir) or not os.path.exists(lbl_dir):\n",
    "        print(f\"[!] {split} directory does not exist, skipping.\")\n",
    "        continue\n",
    "\n",
    "    num_images = len([f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\", \".png\"))])\n",
    "    num_labels = len([f for f in os.listdir(lbl_dir) if f.lower().endswith(\".txt\")])\n",
    "\n",
    "    print(f\"{split.upper()}: Images = {num_images}, Labels = {num_labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e989c",
   "metadata": {},
   "source": [
    "#### Visulaizing yolo format conversion train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d5f1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Paths\n",
    "# yolo_root = r\"dataset-yolo\"\n",
    "# split = \"train\"  # change to \"val\" or \"test\"\n",
    "# img_dir = os.path.join(yolo_root, \"images\", split)\n",
    "# lbl_dir = os.path.join(yolo_root, \"labels\", split)\n",
    "\n",
    "# # Class names (same order as used in conversion)\n",
    "# CLASS_NAMES = [\n",
    "#     \"Handshaking\", \"Hugging\", \"Reading\", \"Drinking\", \"PushingPulling\",\n",
    "#     \"Carrying\", \"Calling\", \"Running\", \"Walking\", \"Lying\",\n",
    "#     \"Sitting\", \"Standing\", \"None\"\n",
    "# ]\n",
    "\n",
    "# # Assign random colors per class\n",
    "# import numpy as np\n",
    "# np.random.seed(42)\n",
    "# COLORS = {cls: tuple(np.random.randint(0,255,3).tolist()) for cls in CLASS_NAMES}\n",
    "\n",
    "# def draw_yolo_bboxes(img, lbl_path):\n",
    "#     h, w, _ = img.shape\n",
    "#     if not os.path.exists(lbl_path):\n",
    "#         return img\n",
    "    \n",
    "#     with open(lbl_path, \"r\") as f:\n",
    "#         lines = f.readlines()\n",
    "    \n",
    "#     for line in lines:\n",
    "#         parts = line.strip().split()\n",
    "#         if len(parts) != 5:\n",
    "#             continue\n",
    "#         cls_id = int(parts[0])\n",
    "#         cx, cy, bw, bh = map(float, parts[1:])\n",
    "        \n",
    "#         # Convert YOLO → pixel coords\n",
    "#         x1 = int((cx - bw/2) * w)\n",
    "#         y1 = int((cy - bh/2) * h)\n",
    "#         x2 = int((cx + bw/2) * w)\n",
    "#         y2 = int((cy + bh/2) * h)\n",
    "        \n",
    "#         cls_name = CLASS_NAMES[cls_id]\n",
    "#         color = COLORS[cls_name]\n",
    "        \n",
    "#         # Draw bbox\n",
    "#         cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "#         cv2.putText(img, cls_name, (x1, max(y1-5,0)),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "#     return img\n",
    "\n",
    "# # Pick random samples\n",
    "# sample_images = random.sample(os.listdir(img_dir), 5)\n",
    "\n",
    "# for img_file in sample_images:\n",
    "#     img_path = os.path.join(img_dir, img_file)\n",
    "#     lbl_path = os.path.join(lbl_dir, img_file.replace(\".jpg\", \".txt\"))\n",
    "    \n",
    "#     img = cv2.imread(img_path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     img = draw_yolo_bboxes(img, lbl_path)\n",
    "    \n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(f\"{split}/{img_file}\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c37798d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Paths\n",
    "# yolo_root = r\"dataset-yolo\"\n",
    "# split = \"val\"  # change to \"val\" or \"test\"\n",
    "# img_dir = os.path.join(yolo_root, \"images\", split)\n",
    "# lbl_dir = os.path.join(yolo_root, \"labels\", split)\n",
    "\n",
    "# # Class names (same order as used in conversion)\n",
    "# CLASS_NAMES = [\n",
    "#     \"Handshaking\", \"Hugging\", \"Reading\", \"Drinking\", \"PushingPulling\",\n",
    "#     \"Carrying\", \"Calling\", \"Running\", \"Walking\", \"Lying\",\n",
    "#     \"Sitting\", \"Standing\", \"None\"\n",
    "# ]\n",
    "\n",
    "# # Assign random colors per class\n",
    "# import numpy as np\n",
    "# np.random.seed(42)\n",
    "# COLORS = {cls: tuple(np.random.randint(0,255,3).tolist()) for cls in CLASS_NAMES}\n",
    "\n",
    "# def draw_yolo_bboxes(img, lbl_path):\n",
    "#     h, w, _ = img.shape\n",
    "#     if not os.path.exists(lbl_path):\n",
    "#         return img\n",
    "    \n",
    "#     with open(lbl_path, \"r\") as f:\n",
    "#         lines = f.readlines()\n",
    "    \n",
    "#     for line in lines:\n",
    "#         parts = line.strip().split()\n",
    "#         if len(parts) != 5:\n",
    "#             continue\n",
    "#         cls_id = int(parts[0])\n",
    "#         cx, cy, bw, bh = map(float, parts[1:])\n",
    "        \n",
    "#         # Convert YOLO → pixel coords\n",
    "#         x1 = int((cx - bw/2) * w)\n",
    "#         y1 = int((cy - bh/2) * h)\n",
    "#         x2 = int((cx + bw/2) * w)\n",
    "#         y2 = int((cy + bh/2) * h)\n",
    "        \n",
    "#         cls_name = CLASS_NAMES[cls_id]\n",
    "#         color = COLORS[cls_name]\n",
    "        \n",
    "#         # Draw bbox\n",
    "#         cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "#         cv2.putText(img, cls_name, (x1, max(y1-5,0)),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "#     return img\n",
    "\n",
    "# # Pick random samples\n",
    "# sample_images = random.sample(os.listdir(img_dir), 5)\n",
    "\n",
    "# for img_file in sample_images:\n",
    "#     img_path = os.path.join(img_dir, img_file)\n",
    "#     lbl_path = os.path.join(lbl_dir, img_file.replace(\".jpg\", \".txt\"))\n",
    "    \n",
    "#     img = cv2.imread(img_path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     img = draw_yolo_bboxes(img, lbl_path)\n",
    "    \n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(f\"{split}/{img_file}\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6ed212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Paths\n",
    "# yolo_root = r\"dataset-yolo\"\n",
    "# split = \"test\"  # change to \"val\" or \"test\"\n",
    "# img_dir = os.path.join(yolo_root, \"images\", split)\n",
    "# lbl_dir = os.path.join(yolo_root, \"labels\", split)\n",
    "\n",
    "# # Class names (same order as used in conversion)\n",
    "# CLASS_NAMES = [\n",
    "#     \"Handshaking\", \"Hugging\", \"Reading\", \"Drinking\", \"PushingPulling\",\n",
    "#     \"Carrying\", \"Calling\", \"Running\", \"Walking\", \"Lying\",\n",
    "#     \"Sitting\", \"Standing\", \"None\"\n",
    "# ]\n",
    "\n",
    "# # Assign random colors per class\n",
    "# import numpy as np\n",
    "# np.random.seed(42)\n",
    "# COLORS = {cls: tuple(np.random.randint(0,255,3).tolist()) for cls in CLASS_NAMES}\n",
    "\n",
    "# def draw_yolo_bboxes(img, lbl_path):\n",
    "#     h, w, _ = img.shape\n",
    "#     if not os.path.exists(lbl_path):\n",
    "#         return img\n",
    "    \n",
    "#     with open(lbl_path, \"r\") as f:\n",
    "#         lines = f.readlines()\n",
    "    \n",
    "#     for line in lines:\n",
    "#         parts = line.strip().split()\n",
    "#         if len(parts) != 5:\n",
    "#             continue\n",
    "#         cls_id = int(parts[0])\n",
    "#         cx, cy, bw, bh = map(float, parts[1:])\n",
    "        \n",
    "#         # Convert YOLO → pixel coords\n",
    "#         x1 = int((cx - bw/2) * w)\n",
    "#         y1 = int((cy - bh/2) * h)\n",
    "#         x2 = int((cx + bw/2) * w)\n",
    "#         y2 = int((cy + bh/2) * h)\n",
    "        \n",
    "#         cls_name = CLASS_NAMES[cls_id]\n",
    "#         color = COLORS[cls_name]\n",
    "        \n",
    "#         # Draw bbox\n",
    "#         cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "#         cv2.putText(img, cls_name, (x1, max(y1-5,0)),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "#     return img\n",
    "\n",
    "# # Pick random samples\n",
    "# sample_images = random.sample(os.listdir(img_dir), 5)\n",
    "\n",
    "# for img_file in sample_images:\n",
    "#     img_path = os.path.join(img_dir, img_file)\n",
    "#     lbl_path = os.path.join(lbl_dir, img_file.replace(\".jpg\", \".txt\"))\n",
    "    \n",
    "#     img = cv2.imread(img_path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     img = draw_yolo_bboxes(img, lbl_path)\n",
    "    \n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(f\"{split}/{img_file}\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aec425",
   "metadata": {},
   "source": [
    "#### Training yolo-v8\n",
    "\n",
    "- training and validation done in another files `yolo-train.py` and `yolo-val.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3328cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14f6d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68f85715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.version.cuda)          # CUDA version PyTorch is using\n",
    "# print(torch.cuda.is_available())   # True if GPU is accessible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33c8b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# def train_yolo_from_scratch(\n",
    "#     data_yaml: str,\n",
    "#     epochs: int,\n",
    "#     imgsz: int,\n",
    "#     batch: int,\n",
    "#     name: str,\n",
    "#     model_variant: str = \"yolov8n.pt\"\n",
    "# ):\n",
    "#     # device = 'cpu'\n",
    "#     device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "#     print(f\"[+] Using device: {device}\")\n",
    "#     print(f\"[+] Training from scratch with model: {model_variant}\")\n",
    "\n",
    "#     model = YOLO(model_variant)\n",
    "\n",
    "#     # Clear memory\n",
    "#     gc.collect()\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     # Start training\n",
    "#     model.train(\n",
    "#         data=data_yaml,\n",
    "#         epochs=epochs,\n",
    "#         imgsz=imgsz,\n",
    "#         batch=batch,\n",
    "#         name=name,\n",
    "#         project=\"runs/train\",\n",
    "#         device=device,\n",
    "#         augment=True,\n",
    "#         degrees=10,\n",
    "#         scale=0.5,\n",
    "#         flipud=0.2,\n",
    "#         fliplr=0.5,\n",
    "#         hsv_h=0.015,\n",
    "#         hsv_s=0.7,\n",
    "#         hsv_v=0.4,\n",
    "#         mosaic=1.0,\n",
    "#         mixup=0.2,\n",
    "#         lr0=0.01,\n",
    "#         lrf=0.01,\n",
    "#         verbose=True,\n",
    "#         patience=15\n",
    "#     )\n",
    "\n",
    "#     print(f\"[+] Training complete. Results saved in 'runs/train/{name}'\")\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba84f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import torch\n",
    "\n",
    "# # Delete 'runs' directory if it exists\n",
    "# runs_dir = \"runs\"\n",
    "# if os.path.exists(runs_dir):\n",
    "#     shutil.rmtree(runs_dir)\n",
    "#     print(f\"Deleted existing '{runs_dir}' directory.\")\n",
    "\n",
    "# # Path to your dataset YAML\n",
    "# data_yaml = \"okutama.yaml\"\n",
    "\n",
    "# # Training parameters\n",
    "# epochs = 1          # Number of training epochs\n",
    "# imgsz = 640         # Image size (YOLO input size)\n",
    "# batch = 16          # Batch size\n",
    "# experiment_name = \"okutama_v8_training\"\n",
    "# model_variant = \"yolov8n.pt\"  # Can also use yolov8s.pt, yolov8m.pt, etc.\n",
    "\n",
    "# # Start training\n",
    "# model = train_yolo_from_scratch(\n",
    "#     data_yaml=data_yaml,\n",
    "#     epochs=epochs,\n",
    "#     imgsz=imgsz,\n",
    "#     batch=batch,\n",
    "#     name=experiment_name,\n",
    "#     model_variant=model_variant\n",
    "# )\n",
    "\n",
    "# # After training, run validation\n",
    "# metrics = model.val()\n",
    "# print(\"Validation metrics:\", metrics)\n",
    "\n",
    "# # Predict on test set\n",
    "# predictions = model.predict(\n",
    "#     source=\"dataset_yolo/images/test\",  # Path to test images\n",
    "#     save=True,                          # Save predictions as images with bounding boxes\n",
    "#     imgsz=imgsz,\n",
    "#     device=0 if torch.cuda.is_available() else 'cpu'\n",
    "# )\n",
    "\n",
    "# print(\"Predictions saved in 'runs/detect/predict'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5aec44",
   "metadata": {},
   "source": [
    "#### Print Train Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f5a3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_best_model(base_dir='runs_yolo/'):\n",
    "    best_paths = list(Path(base_dir).rglob('best.pt'))\n",
    "    if not best_paths:\n",
    "        raise FileNotFoundError(\"No 'best.pt' file found in the 'runs/' directory.\")\n",
    "    \n",
    "    # Optionally, sort by latest modified time\n",
    "    best_paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    print(f\"[+] Found best.pt at: {best_paths[0]}\")\n",
    "    return str(best_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f019d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_results_csv(directory):\n",
    "    \"\"\"Find the results.csv file in the specified directory.\"\"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if 'results.csv' in files:\n",
    "            return os.path.join(root, 'results.csv')\n",
    "    return None\n",
    "\n",
    "def load_results_csv(results_csv_path):\n",
    "    \"\"\"Load the results CSV into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(results_csv_path)\n",
    "\n",
    "def calculate_total_epochs(df):\n",
    "    \"\"\"Calculate the total number of epochs from the DataFrame.\"\"\"\n",
    "    return df['epoch'].max()\n",
    "\n",
    "def calculate_training_loss(epoch_data):\n",
    "    \"\"\"Calculate the total training loss from the given epoch data.\"\"\"\n",
    "    train_box_loss = epoch_data['train/box_loss']\n",
    "    train_cls_loss = epoch_data['train/cls_loss']\n",
    "    train_dfl_loss = epoch_data['train/dfl_loss']\n",
    "    return train_box_loss + train_cls_loss + train_dfl_loss\n",
    "\n",
    "def calculate_validation_loss(epoch_data):\n",
    "    \"\"\"Calculate the total validation loss from the given epoch data.\"\"\"\n",
    "    val_box_loss = epoch_data['val/box_loss']\n",
    "    val_cls_loss = epoch_data['val/cls_loss']\n",
    "    val_dfl_loss = epoch_data['val/dfl_loss']\n",
    "    return val_box_loss + val_cls_loss + val_dfl_loss\n",
    "\n",
    "def print_final_metrics(df):\n",
    "    \"\"\"Print the final metrics for the last epoch.\"\"\"\n",
    "    final_epoch_data = df.iloc[-1]\n",
    "\n",
    "    # Calculate total training and validation loss\n",
    "    train_loss = calculate_training_loss(final_epoch_data)\n",
    "    val_loss = calculate_validation_loss(final_epoch_data)\n",
    "\n",
    "    # Print overall metrics\n",
    "    print(\"\\n========== Final Training Metrics ==========\")\n",
    "    print(f\"Training Loss: {train_loss:.6f}\")\n",
    "    print(f\"Precision: {final_epoch_data['metrics/precision(B)']:.6f}\")\n",
    "    print(f\"Recall: {final_epoch_data['metrics/recall(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5: {final_epoch_data['metrics/mAP50(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5:0.95: {final_epoch_data['metrics/mAP50-95(B)']:.6f}\")\n",
    "\n",
    "    print(\"\\n========== Final Validation Metrics ==========\")\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "\n",
    "def print_csv_metrics(directory):\n",
    "    \"\"\"Main function to process and print final metrics.\"\"\"\n",
    "    # Find the results.csv file\n",
    "    results_csv_path = find_results_csv(directory)\n",
    "    \n",
    "    if not results_csv_path:\n",
    "        print(\"Error: 'results.csv' file not found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found results.csv at: {results_csv_path}\")\n",
    "\n",
    "    # Load results CSV\n",
    "    df = load_results_csv(results_csv_path)\n",
    "\n",
    "    # Get the total number of epochs\n",
    "    total_epochs = calculate_total_epochs(df)\n",
    "    print(f\"Total number of epochs: {total_epochs}\")\n",
    "\n",
    "    print_final_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66ce83e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'results.csv' file not found in the specified directory.\n"
     ]
    }
   ],
   "source": [
    "yolov8 = './runs/train/okutama_v8_training'\n",
    "print_csv_metrics(yolov8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c3d59",
   "metadata": {},
   "source": [
    "#### Print test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fa80c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to predictions folder\n",
    "pred_folder = \"runs/detect/predict\"\n",
    "\n",
    "# Iterate over all prediction subfolders (if any)\n",
    "for root, dirs, files in os.walk(pred_folder):\n",
    "    for f in files:\n",
    "        if f.endswith(\".txt\"):\n",
    "            txt_path = os.path.join(root, f)\n",
    "            with open(txt_path, \"r\") as file:\n",
    "                lines = file.readlines()\n",
    "                if lines:\n",
    "                    print(f\"\\nFile: {txt_path}\")\n",
    "                    for line in lines:\n",
    "                        # Each line: class_id cx cy w h\n",
    "                        parts = line.strip().split()\n",
    "                        class_id = int(parts[0])\n",
    "                        cx, cy, w, h = map(float, parts[1:])\n",
    "                        print(f\"Class {class_id}: cx={cx:.3f}, cy={cy:.3f}, w={w:.3f}, h={h:.3f}\")\n",
    "                else:\n",
    "                    print(f\"\\nFile: {txt_path} -> No detections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e7e070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e232ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95fc652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a93b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
