{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beac06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def download_from_gdrive(drive_url: str, output_path: str) -> str:\n",
    "    try:\n",
    "        print(f\"📥 Downloading from Google Drive...\\nURL: {drive_url}\")\n",
    "        gdown.download(drive_url, output_path, quiet=False)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"❌ Google Drive blocked the download (quota exceeded).\\n\"\n",
    "            \"👉 Try downloading manually in your browser and place the file as 'okutama_action.zip'.\\n\"\n",
    "            f\"Error details: {e}\"\n",
    "        )\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def extract_zip(zip_path: str, extract_to: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract a zip file to a target directory.\n",
    "    Returns the path to the extracted folder.\n",
    "    \"\"\"\n",
    "    if zipfile.is_zipfile(zip_path):\n",
    "        print(f\"📦 Extracting {zip_path} to {extract_to} ...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        os.remove(zip_path)\n",
    "        print(f\"✅ Extraction complete! Files are in: {extract_to}\")\n",
    "        return extract_to\n",
    "    else:\n",
    "        raise ValueError(\"❌ The downloaded file is not a valid zip archive.\")\n",
    "\n",
    "def setup_dataset(drive_url: str, output_zip: str, extract_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    High-level function: Download + Extract dataset.\n",
    "    Returns the dataset root directory.\n",
    "    \"\"\"\n",
    "    zip_path = download_from_gdrive(drive_url, output_zip)\n",
    "    dataset_dir = extract_zip(zip_path, extract_dir)\n",
    "    return dataset_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e350dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# -------------------- Utilities --------------------\n",
    "def get_random_color():\n",
    "    return (random.randint(0,255), random.randint(0,255), random.randint(0,255))\n",
    "\n",
    "def extract_if_zip(path: str) -> str:\n",
    "    if os.path.isfile(path) and path.endswith(\".zip\"):\n",
    "        extract_dir = path[:-4]\n",
    "        if not os.path.exists(extract_dir):\n",
    "            print(f\"Extracting {path} -> {extract_dir}\")\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                z.extractall(extract_dir)\n",
    "        else:\n",
    "            print(f\"Zip already extracted at {extract_dir}\")\n",
    "        return extract_dir\n",
    "    return path\n",
    "\n",
    "def adjust_bbox(xmin, ymin, xmax, ymax, shrink_factor=0.05):\n",
    "    \"\"\"Shrink bbox slightly so it hugs subject more tightly.\"\"\"\n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    if w <= 0 or h <= 0:\n",
    "        return xmin, ymin, xmax, ymax\n",
    "    dx = int(round(w * shrink_factor))\n",
    "    dy = int(round(h * shrink_factor))\n",
    "    return xmin + dx, ymin + dy, xmax - dx, ymax - dy\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    # compute IoU for sanity (not used heavily but handy)\n",
    "    (ax1, ay1, ax2, ay2) = boxA\n",
    "    (bx1, by1, bx2, by2) = boxB\n",
    "    xA = max(ax1, bx1)\n",
    "    yA = max(ay1, by1)\n",
    "    xB = min(ax2, bx2)\n",
    "    yB = min(ay2, by2)\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "    boxAArea = max(0, ax2-ax1) * max(0, ay2-ay1)\n",
    "    boxBArea = max(0, bx2-bx1) * max(0, by2-by1)\n",
    "    denom = float(boxAArea + boxBArea - interArea)\n",
    "    return interArea/denom if denom > 0 else 0.0\n",
    "\n",
    "# -------------------- Parsing & Preprocessing --------------------\n",
    "def parse_annotation_file(anno_path):\n",
    "    \"\"\"\n",
    "    Parse Okutama-style annotation file.\n",
    "    Returns: dict track_id -> list of entries {'frame': int, 'bbox': (x1,y1,x2,y2), 'label': str}\n",
    "    Note: coordinates are returned as floats (could be normalized or pixels).\n",
    "    \"\"\"\n",
    "    tracks = defaultdict(list)\n",
    "    if not os.path.exists(anno_path):\n",
    "        return tracks\n",
    "\n",
    "    with open(anno_path, \"r\") as f:\n",
    "        for ln in f:\n",
    "            parts = ln.strip().split()\n",
    "            if len(parts) < 6:\n",
    "                continue\n",
    "            # Okutama: track_id xmin ymin xmax ymax frame lost occluded generated \"label1\" ...\n",
    "            try:\n",
    "                track_id = int(parts[0])\n",
    "                xmin = float(parts[1]); ymin = float(parts[2])\n",
    "                xmax = float(parts[3]); ymax = float(parts[4])\n",
    "                frame_id = int(parts[5]) - 1   # convert to 0-based\n",
    "                # label may start at index 9 onward; but if not available, fallback to empty\n",
    "                label_tokens = parts[9:] if len(parts) > 9 else []\n",
    "                label = \" \".join([t.strip('\"') for t in label_tokens]) or \"action\"\n",
    "            except Exception:\n",
    "                # if parsing fails, skip line\n",
    "                continue\n",
    "\n",
    "            tracks[track_id].append({\n",
    "                \"frame\": frame_id,\n",
    "                \"bbox\": (xmin, ymin, xmax, ymax),\n",
    "                \"label\": label\n",
    "            })\n",
    "    return tracks\n",
    "\n",
    "def interpolate_and_smooth_tracks(tracks, frame_w, frame_h, frame_count,\n",
    "                                  normalized_threshold=1.01,\n",
    "                                  shrink_factor=0.05, smooth_window=3):\n",
    "    \"\"\"\n",
    "    tracks: dict track_id -> list of entries with 'frame','bbox' (floats), 'label'\n",
    "    Returns: per_frame_annotations: dict frame_idx -> list of (xmin,ymin,xmax,ymax,label,color)\n",
    "    Steps:\n",
    "      - Detect if coords look normalized (<=1.01). If so, scale to pixels using frame_w/frame_h.\n",
    "      - For each track: sort entries by frame, linearly interpolate missing frames between known frames.\n",
    "      - Apply moving-average smoothing on bbox coordinates per track.\n",
    "      - Shrink boxes slightly (shrink_factor), clip to frame bounds, ensure xmin<xmax etc.\n",
    "    \"\"\"\n",
    "    # 1) detect normalization by sampling some boxes\n",
    "    max_coord = 0.0\n",
    "    sample_count = 0\n",
    "    for t_entries in tracks.values():\n",
    "        for e in t_entries[:10]:\n",
    "            (x1,y1,x2,y2) = e[\"bbox\"]\n",
    "            max_coord = max(max_coord, abs(x1), abs(y1), abs(x2), abs(y2))\n",
    "            sample_count += 1\n",
    "            if sample_count >= 200:\n",
    "                break\n",
    "        if sample_count >= 200:\n",
    "            break\n",
    "    normalized = (max_coord <= normalized_threshold)\n",
    "    if normalized:\n",
    "        print(\"Detected normalized coordinates in annotations -> scaling to pixels.\")\n",
    "    else:\n",
    "        print(\"Coordinates appear to be in pixels (no scaling).\")\n",
    "\n",
    "    # colors per label\n",
    "    label_to_color = {}\n",
    "\n",
    "    # per-track processed dicts: track_id -> dict frame -> [x1,y1,x2,y2,label]\n",
    "    processed_tracks = {}\n",
    "\n",
    "    for track_id, entries in tracks.items():\n",
    "        if not entries:\n",
    "            continue\n",
    "        # sort entries by frame\n",
    "        entries_sorted = sorted(entries, key=lambda x: x[\"frame\"])\n",
    "        # convert coords to pixel floats\n",
    "        frame_list = []\n",
    "        coords_list = []\n",
    "        label_list = []\n",
    "        for e in entries_sorted:\n",
    "            f = e[\"frame\"]\n",
    "            x1,y1,x2,y2 = e[\"bbox\"]\n",
    "            if normalized:\n",
    "                x1 = x1 * frame_w\n",
    "                x2 = x2 * frame_w\n",
    "                y1 = y1 * frame_h\n",
    "                y2 = y2 * frame_h\n",
    "            frame_list.append(f)\n",
    "            coords_list.append([float(x1), float(y1), float(x2), float(y2)])\n",
    "            label_list.append(e.get(\"label\",\"action\"))\n",
    "\n",
    "        # build a dict for quick frame->coords\n",
    "        frame_to_coords = {f:coords for f,coords in zip(frame_list, coords_list)}\n",
    "        frame_to_label = {f:lab for f,lab in zip(frame_list, label_list)}\n",
    "\n",
    "        # interpolate between consecutive frames\n",
    "        all_frames = []\n",
    "        all_coords = []\n",
    "        all_labels = []\n",
    "        for i in range(len(frame_list)):\n",
    "            f0 = frame_list[i]\n",
    "            c0 = coords_list[i]\n",
    "            l0 = label_list[i]\n",
    "            all_frames.append(f0)\n",
    "            all_coords.append(c0)\n",
    "            all_labels.append(l0)\n",
    "            if i < len(frame_list)-1:\n",
    "                f1 = frame_list[i+1]\n",
    "                c1 = coords_list[i+1]\n",
    "                if f1 - f0 > 1:\n",
    "                    # linear interpolate intermediate frames\n",
    "                    for t in range(f0+1, f1):\n",
    "                        ratio = (t - f0) / float(f1 - f0)\n",
    "                        interp = [\n",
    "                            c0[j] + ratio * (c1[j] - c0[j]) for j in range(4)\n",
    "                        ]\n",
    "                        all_frames.append(t)\n",
    "                        all_coords.append(interp)\n",
    "                        all_labels.append(l0)  # carry previous label (best-effort)\n",
    "\n",
    "        # Now we have an (unsorted) list of frames+coords; sort them\n",
    "        zipped = list(zip(all_frames, all_coords, all_labels))\n",
    "        zipped_sorted = sorted(zipped, key=lambda x: x[0])\n",
    "        frames_sorted = [z[0] for z in zipped_sorted]\n",
    "        coords_sorted = [z[1] for z in zipped_sorted]\n",
    "        labels_sorted = [z[2] for z in zipped_sorted]\n",
    "\n",
    "        # apply moving-average smoothing per coordinate\n",
    "        w = max(1, int(smooth_window))\n",
    "        half = w // 2\n",
    "        smoothed_coords = []\n",
    "        n = len(coords_sorted)\n",
    "        for i in range(n):\n",
    "            start = max(0, i - half)\n",
    "            end = min(n - 1, i + half)\n",
    "            cnt = end - start + 1\n",
    "            avg = [0.0, 0.0, 0.0, 0.0]\n",
    "            for j in range(start, end+1):\n",
    "                for k in range(4):\n",
    "                    avg[k] += coords_sorted[j][k]\n",
    "            avg = [a / cnt for a in avg]\n",
    "            smoothed_coords.append(avg)\n",
    "\n",
    "        # collect into processed_tracks\n",
    "        proc = {}\n",
    "        for f,c,label in zip(frames_sorted, smoothed_coords, labels_sorted):\n",
    "            proc[f] = {\"bbox\": tuple(c), \"label\": label}\n",
    "        processed_tracks[track_id] = proc\n",
    "\n",
    "        # ensure label->color mapping\n",
    "        for lab in set(labels_sorted):\n",
    "            if lab not in label_to_color:\n",
    "                label_to_color[lab] = get_random_color()\n",
    "\n",
    "    # Build per-frame dict\n",
    "    per_frame = defaultdict(list)  # frame_idx -> list of (xmin,ymin,xmax,ymax,label,color)\n",
    "    for track_id, frame_dict in processed_tracks.items():\n",
    "        for frame_idx in frame_dict.keys():\n",
    "            x1,y1,x2,y2 = frame_dict[frame_idx][\"bbox\"]\n",
    "            label = frame_dict[frame_idx][\"label\"]\n",
    "            # shrink slightly\n",
    "            x1i, y1i, x2i, y2i = adjust_bbox(int(round(x1)), int(round(y1)),\n",
    "                                            int(round(x2)), int(round(y2)), shrink_factor=shrink_factor)\n",
    "            # clip to frame bounds\n",
    "            x1i = max(0, min(x1i, frame_w-1))\n",
    "            x2i = max(0, min(x2i, frame_w-1))\n",
    "            y1i = max(0, min(y1i, frame_h-1))\n",
    "            y2i = max(0, min(y2i, frame_h-1))\n",
    "            # ensure proper ordering and at least 2 pixels size\n",
    "            if x2i <= x1i + 1:\n",
    "                x2i = min(frame_w-1, x1i + 2)\n",
    "            if y2i <= y1i + 1:\n",
    "                y2i = min(frame_h-1, y1i + 2)\n",
    "\n",
    "            per_frame[frame_idx].append((x1i, y1i, x2i, y2i, label, label_to_color.get(label, get_random_color())))\n",
    "\n",
    "    return per_frame\n",
    "\n",
    "# -------------------- Main annotation pipeline --------------------\n",
    "def annotate_videos(data_dir: str, source_dir: str, output_dir: str,\n",
    "                    shrink_factor=0.05, smooth_window=3, n_samples=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    samples_dir = os.path.join(output_dir)\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "\n",
    "    # If source is zip -> extract, else keep as-is\n",
    "    source_dir = extract_if_zip(source_dir)\n",
    "\n",
    "    # find videos\n",
    "    video_files = glob.glob(os.path.join(source_dir, \"*.mov\"))\n",
    "    if not video_files:\n",
    "        print(f\"No .mov files found in '{source_dir}'.\")\n",
    "        return\n",
    "    print(f\"Found {len(video_files)} videos in '{source_dir}'.\")\n",
    "\n",
    "    for video_path in video_files:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        base_name = os.path.basename(video_path)\n",
    "        print(f\"Processing video: {base_name}\")\n",
    "        file_stem = os.path.splitext(base_name)[0]\n",
    "        anno_path = os.path.join(source_dir, f\"{file_stem}.txt\")\n",
    "        out_video_path = os.path.join(output_dir, f\"{file_stem}_annotated.mp4\")\n",
    "\n",
    "        if not os.path.exists(anno_path):\n",
    "            print(f\"Annotation file not found for {base_name} -> skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Parse annotation file into per-track data\n",
    "        tracks = parse_annotation_file(anno_path)\n",
    "        if not tracks:\n",
    "            print(f\"No valid tracks parsed from {anno_path}. Skipping.\")\n",
    "            continue\n",
    "        print(f\"Parsed {len(tracks)} tracks from annotation file.\")\n",
    "\n",
    "        # Open video to get dimensions and frame_count\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Could not open video: {video_path}. Skipping.\")\n",
    "            continue\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "        frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(f\"🎞 Video: FPS={fps:.2f}, Size=({frame_w}x{frame_h}), Frames={frame_count}\")\n",
    "\n",
    "        # Preprocess: interpolate + smooth + convert to per-frame annotations\n",
    "        per_frame_ann = interpolate_and_smooth_tracks(tracks, frame_w, frame_h, frame_count,\n",
    "                                                      normalized_threshold=1.01,\n",
    "                                                      shrink_factor=shrink_factor,\n",
    "                                                      smooth_window=smooth_window)\n",
    "        print(f\"After smoothing/interpolation: {len(per_frame_ann)} frames have annotations.\")\n",
    "\n",
    "        # Prepare VideoWriter\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(out_video_path, fourcc, fps, (frame_w, frame_h))\n",
    "\n",
    "        # choose sample frames intelligently: prioritize frames that have annotations\n",
    "        frames_with_ann = sorted(per_frame_ann.keys())\n",
    "        if frames_with_ann:\n",
    "            chosen = frames_with_ann.copy()\n",
    "            random.shuffle(chosen)\n",
    "            sample_frames = set(chosen[:min(n_samples, len(chosen))])\n",
    "        else:\n",
    "            # fallback: random frames from video\n",
    "            sample_frames = set(random.sample(range(frame_count), min(n_samples, max(1, frame_count))))\n",
    "\n",
    "        # annotate and write\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        frame_idx = 0\n",
    "        saved_samples = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if frame_idx in per_frame_ann:\n",
    "                for (xmin, ymin, xmax, ymax, label, color) in per_frame_ann[frame_idx]:\n",
    "                    cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "                    text = label\n",
    "                    (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                    text_bg_tl = (xmin, max(0, ymin - th - 8))\n",
    "                    text_bg_br = (xmin + tw, ymin)\n",
    "                    cv2.rectangle(frame, text_bg_tl, text_bg_br, color, -1)\n",
    "                    cv2.putText(frame, text, (xmin, max(15, ymin - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "            if frame_idx in sample_frames:\n",
    "                sample_file = os.path.join(samples_dir, f\"{file_stem}_sample_{frame_idx}.jpg\")\n",
    "                cv2.imwrite(sample_file, frame)\n",
    "                saved_samples += 1\n",
    "                print(f\"Saved sample {saved_samples}: {sample_file}\")\n",
    "\n",
    "            frame_idx += 1\n",
    "            if frame_idx % 200 == 0:\n",
    "                print(f\"  ... processed {frame_idx}/{frame_count} frames ...\")\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Annotated video written to: {out_video_path}\")\n",
    "\n",
    "        # save a preview (first frame) also\n",
    "        cap_preview = cv2.VideoCapture(out_video_path)\n",
    "        ret, pframe = cap_preview.read()\n",
    "        if ret:\n",
    "            preview_file = os.path.join(output_dir, f\"{file_stem}_preview.jpg\")\n",
    "            cv2.imwrite(preview_file, pframe)\n",
    "            print(f\"Preview saved: {preview_file}\")\n",
    "        cap_preview.release()\n",
    "\n",
    "    print(\"\\nAll done. Check the outputs and samples folder for results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a923a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c7298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRIVE_URL = \"https://drive.google.com/uc?id=1fJshjFiK0LNfrLHbGPmhYm9b8WzUKDtY\"\n",
    "# OUTPUT_ZIP = \"okutama_action.zip\"\n",
    "# EXTRACT_DIR = \"okutama_action\"\n",
    "\n",
    "# dataset_path = setup_dataset(DRIVE_URL, OUTPUT_ZIP, EXTRACT_DIR)\n",
    "# print(f\"Dataset ready at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec71ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip already extracted at okutama-action\\Sample\n",
      "Found 1 videos in 'okutama-action\\Sample'.\n",
      "\n",
      "============================================================\n",
      "Processing video: 1.1.1.mov\n",
      "Parsed 100 tracks from annotation file.\n",
      "🎞 Video: FPS=29.97, Size=(3840x2160), Frames=2272\n",
      "Coordinates appear to be in pixels (no scaling).\n",
      "After smoothing/interpolation: 2272 frames have annotations.\n",
      "Saved sample 1: okutama-action-outputs\\sample-visualize\\1.1.1_sample_61.jpg\n",
      "Saved sample 2: okutama-action-outputs\\sample-visualize\\1.1.1_sample_72.jpg\n",
      "  ... processed 200/2272 frames ...\n",
      "Saved sample 3: okutama-action-outputs\\sample-visualize\\1.1.1_sample_232.jpg\n",
      "Saved sample 4: okutama-action-outputs\\sample-visualize\\1.1.1_sample_357.jpg\n",
      "  ... processed 400/2272 frames ...\n",
      "  ... processed 600/2272 frames ...\n",
      "Saved sample 5: okutama-action-outputs\\sample-visualize\\1.1.1_sample_620.jpg\n",
      "Saved sample 6: okutama-action-outputs\\sample-visualize\\1.1.1_sample_645.jpg\n",
      "Saved sample 7: okutama-action-outputs\\sample-visualize\\1.1.1_sample_683.jpg\n",
      "  ... processed 800/2272 frames ...\n",
      "Saved sample 8: okutama-action-outputs\\sample-visualize\\1.1.1_sample_977.jpg\n",
      "  ... processed 1000/2272 frames ...\n",
      "  ... processed 1200/2272 frames ...\n",
      "Saved sample 9: okutama-action-outputs\\sample-visualize\\1.1.1_sample_1290.jpg\n",
      "  ... processed 1400/2272 frames ...\n",
      "  ... processed 1600/2272 frames ...\n",
      "Saved sample 10: okutama-action-outputs\\sample-visualize\\1.1.1_sample_1759.jpg\n",
      "  ... processed 1800/2272 frames ...\n",
      "  ... processed 2000/2272 frames ...\n",
      "  ... processed 2200/2272 frames ...\n",
      "Annotated video written to: okutama-action-outputs\\sample-visualize\\1.1.1_annotated.mp4\n",
      "Preview saved: okutama-action-outputs\\sample-visualize\\1.1.1_preview.jpg\n",
      "\n",
      "All done. Check the outputs and samples folder for results.\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Example Usage -----------------\n",
    "data_dir = \"okutama-action\"\n",
    "main_output_dir = \"okutama-action-outputs\"\n",
    "source_dir = os.path.join(data_dir, \"Sample.zip\")   # or \"Sample\" if already extracted\n",
    "output_dir = os.path.join(main_output_dir, \"sample-visualize\")\n",
    "\n",
    "annotate_videos(data_dir, source_dir, output_dir, shrink_factor=0.08, smooth_window=3, n_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954361ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ce8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf000c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda89c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b6f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e55a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
